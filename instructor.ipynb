{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af4783c",
   "metadata": {},
   "source": [
    "## Instructor and responsemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c071a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "# built-in libraries\n",
    "from typing import TypeVar, Literal, Any\n",
    "from pydantic import BaseModel, create_model\n",
    "\n",
    "# litellm libraries\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from litellm.types.utils import ModelResponse, Message\n",
    "\n",
    "#instructor\n",
    "from instructor import from_litellm, Mode\n",
    "\n",
    "# misc libraries\n",
    "from decouple import config\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb9e47",
   "metadata": {},
   "source": [
    "setting up the watsonx credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24439222",
   "metadata": {},
   "outputs": [],
   "source": [
    "WX_API_KEY = config(\"WX_API_KEY\")\n",
    "WX_PROJECT_ID = config(\"WX_PROJECT_ID\")\n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\"\n",
    "WX_MODEL_ID = \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139fb50",
   "metadata": {},
   "source": [
    "Initializing our instructor and creating our response model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c48150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseResponse(BaseModel):\n",
    "    answer: str\n",
    "ResponseType = TypeVar('ResponseType', bound=BaseModel)\n",
    "\n",
    "\n",
    "\n",
    "class LLMCaller:\n",
    "\n",
    "    def __init__(self, api_key: str, project_id: str, api_url: str, model_id: str, params: dict[str, Any]):\n",
    "        self.api_key = WX_API_KEY\n",
    "        self.project_id = WX_PROJECT_ID\n",
    "        self.api_url = WX_API_URL\n",
    "        self.model_id = WX_MODEL_ID\n",
    "        self.params = params\n",
    "\n",
    "        # Boilerplate: Configure LiteLLM to drop unsupported parameters for Watsonx.ai\n",
    "        litellm.drop_params = True\n",
    "        # Boilerplate: Create an Instructor client for pydantic-based interactions with the LLM\n",
    "        self.client = from_litellm(completion, mode=Mode.JSON)\n",
    "\n",
    "    def create_response_model(self, title: str, fields: dict) -> ResponseType:\n",
    "        \"\"\" Dynamically creates a Pydantic response model for the LLM's output.\n",
    "        Args:\n",
    "            title (str): The name of the response model.\n",
    "            fields (dict): A dictionary defining the fields of the response model.\n",
    "                           Keys are field names, and values are tuples of (type, Field).\n",
    "\n",
    "        Returns:\n",
    "            ResponseType: A dynamically created Pydantic model class.\n",
    "        \"\"\"\n",
    "        return create_model(title, **fields, __base__=BaseResponse)\n",
    "\n",
    "    def invoke(self, prompt: str, response_model: ResponseType = BaseResponse, **kwargs) -> ResponseType:\n",
    "        \"\"\" Sends a prompt to the LLM and retrieves a structured response.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt to send to the LLM.\n",
    "            response_model (ResponseType): The Pydantic model to structure the LLM's response.\n",
    "                                           Defaults to BaseResponse.\n",
    "            **kwargs: Additional arguments to pass to the LLM client.\n",
    "\n",
    "        Returns:\n",
    "            ResponseType: The structured response from the LLM, parsed into the specified response model.\n",
    "        \"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt + \"\\n\\n\" + f\"Provide your answer as an object of {type(response_model)}\",\n",
    "                    # notice how we hardcode instructions on the responde model type for the llm\n",
    "                    # so we don't have to repeat it in the prompt\n",
    "                }\n",
    "            ],\n",
    "            project_id=self.project_id,\n",
    "            apikey=self.api_key,\n",
    "            api_base=self.api_url,\n",
    "            response_model=response_model,\n",
    "            **kwargs\n",
    "        )\n",
    "        return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
