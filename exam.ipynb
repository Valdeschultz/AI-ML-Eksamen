{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf97061",
   "metadata": {},
   "source": [
    "## Classification of emotions in the Go emotions dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fce2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf02ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langgraph.graph import START, StateGraph\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6396b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from litellm import completion\n",
    "\n",
    "import instructor\n",
    "from instructor import Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1543a3c",
   "metadata": {},
   "source": [
    "Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f5d4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for the data\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf22df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading goemotions_1 dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m target_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(target_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#URLs for the Go emotions dataset files\n",
    "urls = {\n",
    "    'goemotions_1': 'https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv',\n",
    "    'goemotions_2': 'https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv',\n",
    "    'goemotions_3': 'https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv'\n",
    "}\n",
    "\n",
    "# Download each file\n",
    "for name, url in urls.items():\n",
    "    target_path = data_dir / f\"{name}.csv\"\n",
    "    \n",
    "    if not target_path.exists():\n",
    "        print(f\"Downloading {name} dataset...\")\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(target_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Successfully downloaded {name} dataset to {target_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {name} dataset. Status code: {response.status_code}\")\n",
    "    else:\n",
    "        print(f\"{name} dataset already exists at {target_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30129ddf",
   "metadata": {},
   "source": [
    "loading the data and combining the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f70e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the goemotions_1.csv file into a pandas DataFrame\n",
    "goemotions_1_df = pd.read_csv(\"data/goemotions1.csv\")\n",
    "goemotions_2_df = pd.read_csv(\"data/goemotions2.csv\")\n",
    "goemotions_3_df = pd.read_csv(\"data/goemotions3.csv\")\n",
    "\n",
    "\n",
    "# Combine the datasets into a single DataFrame\n",
    "combined_df = pd.concat([goemotions_1_df, goemotions_2_df, goemotions_3_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c302c420",
   "metadata": {},
   "source": [
    "data preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d82c826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in combined_df:\n",
      "- text\n",
      "- id\n",
      "- author\n",
      "- subreddit\n",
      "- link_id\n",
      "- parent_id\n",
      "- created_utc\n",
      "- rater_id\n",
      "- example_very_unclear\n",
      "- admiration\n",
      "- amusement\n",
      "- anger\n",
      "- annoyance\n",
      "- approval\n",
      "- caring\n",
      "- confusion\n",
      "- curiosity\n",
      "- desire\n",
      "- disappointment\n",
      "- disapproval\n",
      "- disgust\n",
      "- embarrassment\n",
      "- excitement\n",
      "- fear\n",
      "- gratitude\n",
      "- grief\n",
      "- joy\n",
      "- love\n",
      "- nervousness\n",
      "- optimism\n",
      "- pride\n",
      "- realization\n",
      "- relief\n",
      "- remorse\n",
      "- sadness\n",
      "- surprise\n",
      "- neutral\n"
     ]
    }
   ],
   "source": [
    "# Print a nicely formatted list of all features in combined_df\n",
    "print(\"Features in combined_df:\")\n",
    "for feature in combined_df.columns:\n",
    "    print(f\"- {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a417ec1",
   "metadata": {},
   "source": [
    "Removal of features with no predictive value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71fbcb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the specified features from combined_df\n",
    "features_to_remove = ['author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear']\n",
    "combined_df = combined_df.drop(columns=features_to_remove)\n",
    "\n",
    "#remove duplicates\n",
    "#full obeservation duplicates  \n",
    "combined_df = combined_df.drop_duplicates()\n",
    "#text duplicates\n",
    "combined_df = combined_df.drop_duplicates(subset='text')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9973b5",
   "metadata": {},
   "source": [
    "Checking new Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "535b2c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in combined_df:\n",
      "- text\n",
      "- id\n",
      "- admiration\n",
      "- amusement\n",
      "- anger\n",
      "- annoyance\n",
      "- approval\n",
      "- caring\n",
      "- confusion\n",
      "- curiosity\n",
      "- desire\n",
      "- disappointment\n",
      "- disapproval\n",
      "- disgust\n",
      "- embarrassment\n",
      "- excitement\n",
      "- fear\n",
      "- gratitude\n",
      "- grief\n",
      "- joy\n",
      "- love\n",
      "- nervousness\n",
      "- optimism\n",
      "- pride\n",
      "- realization\n",
      "- relief\n",
      "- remorse\n",
      "- sadness\n",
      "- surprise\n",
      "- neutral\n",
      "Shape of combined_df: (57732, 30)\n"
     ]
    }
   ],
   "source": [
    "#cheking to see if the features are removed or not\n",
    "print(\"Features in combined_df:\")\n",
    "for feature in combined_df.columns:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "#see shape of combined_df\n",
    "print(\"Shape of combined_df:\", combined_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6483e227",
   "metadata": {},
   "source": [
    "Splitting the dataset into train test & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39f29e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 46185\n",
      "Validation size: 5773\n",
      "Test size: 5774\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into train (80%) and temp (20%)\n",
    "train_df, temp_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temp data into validation (50% of temp) and test (50% of temp)\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(validation_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7797f2",
   "metadata": {},
   "source": [
    "## building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d3af7",
   "metadata": {},
   "source": [
    "Connecting to WatsonX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a448d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "WX_API_KEY = os.getenv(\"WX_API_KEY\")\n",
    "WX_PROJECT_ID_RAG = os.getenv(\"WX_PROJECT_ID_RAG\")\n",
    "WX_URL = os.getenv(\"WX_URL\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77e2fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxLLM(\n",
    "\n",
    "        model_id=\"mistralai/mistral-large\",\n",
    "        url=WX_URL,\n",
    "        apikey=WX_API_KEY,\n",
    "        project_id=WX_PROJECT_ID_RAG,\n",
    "        username=\"makr21ag@student.cbs.dk\",\n",
    "\n",
    "        params={\n",
    "            GenParams.DECODING_METHOD: \"greedy\",\n",
    "            GenParams.TEMPERATURE: 0,\n",
    "            GenParams.MIN_NEW_TOKENS: 5,\n",
    "            GenParams.MAX_NEW_TOKENS: 1_000,\n",
    "            GenParams.REPETITION_PENALTY: 1.2\n",
    "        }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aaaed3",
   "metadata": {},
   "source": [
    "Testing the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18216d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I love dogs, but I don‚Äôt have a particular favourite breed because they are all so different in their own way! However, if you were to ask me what my dream pet would be (if money was no object), then I think that the answer might surprise some people: an Alaskan Malamute or Siberian Husky puppy ‚Äì preferably one with blue eyes like those seen on Game Of Thrones üòâ These two types of huskies look very similar at first glance; however there are subtle differences between them which make each unique from another type such as size/weight etcetera‚Ä¶ But both share many traits including being friendly towards humans while also having strong pack instincts making these animals great companions for families who want something more than just ‚Äúman‚Äôs best friend‚Äù. They require lots exercise though due to high energy levels so owners need plenty space outdoors where possible too keep fit themselves during walks together every day otherwise could become destructive indoors instead!"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\n",
    "    \"Describe your favorite breed of dog and why it is your favorite.\"\n",
    "):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74715054",
   "metadata": {},
   "source": [
    "We're saving our training split as a csv-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efae3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training split saved to data/train_split.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the training split into a CSV file in the data folder\n",
    "train_file_path = data_dir / \"train_split.csv\"\n",
    "train_df.to_csv(train_file_path, index=False)\n",
    "print(f\"Training split saved to {train_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ecae1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: lol. good one dude. you really showed me. great argument. really showing off your intellectual prowess. keep up the good work!\n",
      "id: eevpt1e\n",
      "admiration: 0\n",
      "amusement: 1\n",
      "anger: 0\n",
      "annoyance: 0\n",
      "approval: 0\n",
      "caring: 0\n",
      "confusion: 0\n",
      "curiosity: 0\n",
      "desire: 0\n",
      "disappointment: 0\n",
      "disapproval: 0\n",
      "disgust: 0\n",
      "embarrassment: 0\n",
      "excitement: 0\n",
      "fear: 0\n",
      "gratitude: 0\n",
      "grief: 0\n",
      "joy: 0\n",
      "love: 0\n",
      "nervousness: 0\n",
      "optimism: 0\n",
      "pride: 0\n",
      "realization: 0\n",
      "relief: 0\n",
      "remorse: 0\n",
      "sadness: 0\n",
      "surprise: 0\n",
      "neutral: 0\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "# point at your CSV file; it will emit one Document per row,\n",
    "# with page_content = the concatenated row and metadata = the rest of the columns\n",
    "loader = CSVLoader(\n",
    "    file_path=data_dir / \"train_split.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    csv_args={\"delimiter\": \",\"},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[0].page_content)\n",
    "# each doc.page_content is one row‚Äôs text, and doc.metadata contains the 28 emotion flags\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aiml25-ma3)",
   "language": "python",
   "name": "aiml25-ma3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
