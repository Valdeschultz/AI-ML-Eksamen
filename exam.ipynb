{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf97061",
   "metadata": {},
   "source": [
    "## Classification of emotions in the Go emotions dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fce2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf02ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langgraph.graph import START, StateGraph\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6396b9e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 1980: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m completion\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m##from litellm import completion\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m##import instructor\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m##from instructor import Mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Valdemar Schultz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\litellm\\__init__.py:762\u001b[0m\n\u001b[0;32m    759\u001b[0m openai_image_generation_models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdall-e-2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdall-e-3\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtimeout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timeout\n\u001b[1;32m--> 762\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcost_calculator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m completion_cost\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlitellm_core_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlitellm_logging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logging, modify_integration\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlitellm_core_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mget_llm_provider_logic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_llm_provider\n",
      "File \u001b[1;32mc:\\Users\\Valdemar Schultz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\litellm\\cost_calculator.py:19\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     DEFAULT_MAX_LRU_CACHE_SIZE,\n\u001b[0;32m     14\u001b[0m     DEFAULT_REPLICATE_GPU_PRICE_PER_SECOND,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlitellm_core_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_cost_calc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtool_call_cost_tracking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     StandardBuiltInToolCostTracking,\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlitellm_core_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_cost_calc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     _generic_cost_per_character,\n\u001b[0;32m     21\u001b[0m     generic_cost_per_token,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manthropic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcost_calculation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     cost_per_token \u001b[38;5;28;01mas\u001b[39;00m anthropic_cost_per_token,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcost_calculation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     cost_per_token \u001b[38;5;28;01mas\u001b[39;00m azure_openai_cost_per_token,\n\u001b[0;32m     28\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Valdemar Schultz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\litellm\\litellm_core_utils\\llm_cost_calc\\utils.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m verbose_logger\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelInfo, Usage\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model_info\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_above_128k\u001b[39m(tokens: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m128000\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Valdemar Schultz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\litellm\\utils.py:188\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# PythonÂ 3.9+\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m resources\u001b[38;5;241m.\u001b[39mfiles(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm.litellm_core_utils.tokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mjoinpath(\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic_tokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m     )\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 188\u001b[0m         json_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m resources\u001b[38;5;241m.\u001b[39mopen_text(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm.litellm_core_utils.tokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic_tokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\Valdemar Schultz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Valdemar Schultz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 1980: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "from litellm import completion\n",
    "\n",
    "##import instructor\n",
    "##from instructor import Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1543a3c",
   "metadata": {},
   "source": [
    "Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f5d4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for the data\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf22df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading goemotions_1 dataset...\n",
      "Successfully downloaded goemotions_1 dataset to data\\full_dataset\\goemotions_1.csv\n",
      "Downloading goemotions_2 dataset...\n",
      "Successfully downloaded goemotions_2 dataset to data\\full_dataset\\goemotions_2.csv\n",
      "Downloading goemotions_3 dataset...\n",
      "Successfully downloaded goemotions_3 dataset to data\\full_dataset\\goemotions_3.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#URLs for the Go emotions dataset files\n",
    "urls = {\n",
    "    'goemotions_1': 'https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv',\n",
    "    'goemotions_2': 'https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv',\n",
    "    'goemotions_3': 'https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv'\n",
    "}\n",
    "\n",
    "# Download each file\n",
    "for name, url in urls.items():\n",
    "    target_path = data_dir / f\"{name}.csv\"\n",
    "    \n",
    "    if not target_path.exists():\n",
    "        print(f\"Downloading {name} dataset...\")\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(target_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Successfully downloaded {name} dataset to {target_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {name} dataset. Status code: {response.status_code}\")\n",
    "    else:\n",
    "        print(f\"{name} dataset already exists at {target_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30129ddf",
   "metadata": {},
   "source": [
    "loading the data and combining the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f70e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the goemotions_1.csv file into a pandas DataFrame\n",
    "goemotions_1_df = pd.read_csv(\"data/goemotions1.csv\")\n",
    "goemotions_2_df = pd.read_csv(\"data/goemotions2.csv\")\n",
    "goemotions_3_df = pd.read_csv(\"data/goemotions3.csv\")\n",
    "\n",
    "\n",
    "# Combine the datasets into a single DataFrame\n",
    "combined_df = pd.concat([goemotions_1_df, goemotions_2_df, goemotions_3_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c302c420",
   "metadata": {},
   "source": [
    "data preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d82c826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in combined_df:\n",
      "- text\n",
      "- id\n",
      "- admiration\n",
      "- amusement\n",
      "- anger\n",
      "- annoyance\n",
      "- approval\n",
      "- caring\n",
      "- confusion\n",
      "- curiosity\n",
      "- desire\n",
      "- disappointment\n",
      "- disapproval\n",
      "- disgust\n",
      "- embarrassment\n",
      "- excitement\n",
      "- fear\n",
      "- gratitude\n",
      "- grief\n",
      "- joy\n",
      "- love\n",
      "- nervousness\n",
      "- optimism\n",
      "- pride\n",
      "- realization\n",
      "- relief\n",
      "- remorse\n",
      "- sadness\n",
      "- surprise\n",
      "- neutral\n"
     ]
    }
   ],
   "source": [
    "# Print a nicely formatted list of all features in combined_df\n",
    "print(\"Features in combined_df:\")\n",
    "for feature in combined_df.columns:\n",
    "    print(f\"- {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a417ec1",
   "metadata": {},
   "source": [
    "Removal of features with no predictive value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbcb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the specified features from combined_df\n",
    "features_to_remove = ['author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear']\n",
    "combined_df = combined_df.drop(columns=features_to_remove)\n",
    "\n",
    "#remove duplicates\n",
    "#full obeservation duplicates  \n",
    "combined_df = combined_df.drop_duplicates()\n",
    "#text duplicates\n",
    "combined_df = combined_df.drop_duplicates(subset='text')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9973b5",
   "metadata": {},
   "source": [
    "Checking new Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "535b2c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in combined_df:\n",
      "- text\n",
      "- id\n",
      "- admiration\n",
      "- amusement\n",
      "- anger\n",
      "- annoyance\n",
      "- approval\n",
      "- caring\n",
      "- confusion\n",
      "- curiosity\n",
      "- desire\n",
      "- disappointment\n",
      "- disapproval\n",
      "- disgust\n",
      "- embarrassment\n",
      "- excitement\n",
      "- fear\n",
      "- gratitude\n",
      "- grief\n",
      "- joy\n",
      "- love\n",
      "- nervousness\n",
      "- optimism\n",
      "- pride\n",
      "- realization\n",
      "- relief\n",
      "- remorse\n",
      "- sadness\n",
      "- surprise\n",
      "- neutral\n",
      "Shape of combined_df: (57732, 30)\n"
     ]
    }
   ],
   "source": [
    "#cheking to see if the features are removed or not\n",
    "print(\"Features in combined_df:\")\n",
    "for feature in combined_df.columns:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "#see shape of combined_df\n",
    "print(\"Shape of combined_df:\", combined_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6483e227",
   "metadata": {},
   "source": [
    "Splitting the dataset into train test & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39f29e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 46185\n",
      "Validation size: 5773\n",
      "Test size: 5774\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into train (80%) and temp (20%)\n",
    "train_df, temp_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temp data into validation (50% of temp) and test (50% of temp)\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(validation_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7797f2",
   "metadata": {},
   "source": [
    "## building the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34155c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Valdemar Schultz\\Desktop\\AI-ML Eksamen\n",
      ".env file exists: True\n",
      "Contents of .env file:\n",
      "WX_PROJECT_ID_RAG=\"5b56ca80-258f-4dca-b3b9-413dd0495235\"\n",
      "WX_API_KEY=\"vKoe2CuO-u0WmIydpGGMtvzgAKb_1jIqq7LeAIWSwcXa\"\n",
      "WX_URL=\"https://us-south.ml.cloud.ibm.com\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Print the current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if .env file exists\n",
    "env_path = os.path.join(os.getcwd(), '.env')\n",
    "print(f\".env file exists: {os.path.exists(env_path)}\")\n",
    "\n",
    "# Try to read the .env file directly to see its contents\n",
    "try:\n",
    "    with open('.env', 'r') as f:\n",
    "        env_contents = f.read()\n",
    "        print(\"Contents of .env file:\")\n",
    "        print(env_contents)\n",
    "        \n",
    "        # Parse the .env file and set variables\n",
    "        for line in env_contents.splitlines():\n",
    "            if '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip().strip('\"')\n",
    "                globals()[key] = value  # Dynamically create variables\n",
    "except FileNotFoundError:\n",
    "    print(\".env file not found. Make sure it's in the correct location.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a448d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WX_API_KEY: vKoe2CuO-u0WmIydpGGMtvzgAKb_1jIqq7LeAIWSwcXa\n",
      "WX_PROJECT_ID_RAG: 5b56ca80-258f-4dca-b3b9-413dd0495235\n",
      "WX_URL: https://us-south.ml.cloud.ibm.com\n"
     ]
    }
   ],
   "source": [
    "print(\"WX_API_KEY:\", WX_API_KEY)\n",
    "print(\"WX_PROJECT_ID_RAG:\", WX_PROJECT_ID_RAG)\n",
    "print(\"WX_URL:\", WX_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2fe04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
