{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b5634f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import TypeVar, Any\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from instructor import from_litellm, Mode\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57b264",
   "metadata": {},
   "source": [
    "# Load definitions from the Oxford dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "afd0ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/oxford_ekman_emotions.csv\")\n",
    "definitions = \"\\n\".join(f\"{row['emotion']}: {row['definition']}\" for _, row in df.iterrows())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35149508",
   "metadata": {},
   "source": [
    "# Create a system prompt for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a3c21459",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You’re given a piece of text.  Your job is to pick **all** emotions (one or more) from this list:\n",
    "  [joy, anger, sadness, fear, disgust, surprise, neutral]\n",
    "\n",
    "Respond with a **JSON array** of exact emotion keywords.  \n",
    "– If more than one emotion fits, list them all.  \n",
    "– If none apply, return an empty array: []  \n",
    "– Don’t include any explanation or extra text.\n",
    "\n",
    "Emotion definitions:\n",
    "{definitions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85246fc1",
   "metadata": {},
   "source": [
    "# Creating instructor client from litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "44267459",
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.drop_params = True\n",
    "client = from_litellm(completion, mode=Mode.JSON)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09011bdd",
   "metadata": {},
   "source": [
    "# Define EkmanEmotion as a Literal of valid emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4fdbbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EkmanEmotion = Literal[\"Anger\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f904887",
   "metadata": {},
   "source": [
    "# Define the EmotionPrediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6c9956d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionPrediction(BaseModel):\n",
    "    emotion: EkmanEmotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79401575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response_model = create_model(\n",
    "#    \"MyResponseModel\", \n",
    "#    reasoning=(str, Field(description=\"The short reasoning behind the answer\")),\n",
    "#    answer=(str, Field(description=\"Your answer to the question\")),\n",
    "#    __base__=BaseModel\n",
    "#) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a1e09",
   "metadata": {},
   "source": [
    "# Initialize the LLM client with JSON response mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a7295720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a base response model using Pydantic.\n",
    "class BaseResponse(BaseModel):\n",
    "    \"\"\"A default response model that stores a list of predicted Ekman emotions. We will use this to predict the emotions of a review.\"\"\"\n",
    "    answer: str\n",
    "\n",
    "# Define a generic type for later use, bounded to Pydantic BaseModel\n",
    "ResponseType = TypeVar(\"ResponseType\", bound=BaseModel)\n",
    "\n",
    "class LLMCaller:\n",
    "    \"\"\"\n",
    "    A class to interact with a Large Language Model (LLM)\n",
    "    using the LiteLLM and Instructor libraries.\n",
    "    \n",
    "    Designed to send prompts and receive structured responses\n",
    "    as Pydantic models (e.g., predicted emotions).\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key: str, project_id: str, api_url: str, model_id: str, params: dict[str, Any]):\n",
    "        \"\"\"Initializes the LLMCaller with Watsonx credentials and configuration.\"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.project_id = project_id\n",
    "        self.api_url = api_url\n",
    "        self.model_id = model_id\n",
    "        self.params = params\n",
    "\n",
    "        litellm.drop_params = True\n",
    "        self.client = from_litellm(completion, mode=Mode.JSON)\n",
    "\n",
    "    def create_response_model(self, title: str, fields: dict) -> ResponseType:\n",
    "        \"\"\" Dynamically creates a Pydantic response model for the LLM's output.\n",
    "        Args:\n",
    "            title (str): The name of the response model.\n",
    "            fields (dict): A dictionary defining the fields of the response model.\n",
    "                           Keys are field names, and values are tuples of (type, Field).\n",
    "\n",
    "        Returns:\n",
    "            ResponseType: A dynamically created Pydantic model class.\n",
    "        \"\"\"\n",
    "        return create_model(title, **fields, __base__=BaseResponse)\n",
    "\n",
    "    def invoke(self, prompt: str, response_model: ResponseType = BaseResponse, **kwargs) -> ResponseType:\n",
    "        \"\"\" Sends a prompt to the LLM and retrieves a structured response.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt to send to the LLM.\n",
    "            response_model (ResponseType): The Pydantic model to structure the LLM's response.\n",
    "                                           Defaults to BaseResponse.\n",
    "            **kwargs: Additional arguments to pass to the LLM client.\n",
    "\n",
    "        Returns:\n",
    "            ResponseType: The structured response from the LLM, parsed into the specified response model.\n",
    "        \"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt + \"\\n\\nRespond using this structure: \" + str(response_model.__annotations__)\n",
    "            }],\n",
    "            project_id=self.project_id,\n",
    "            apikey=self.api_key,\n",
    "            api_base=self.api_url,\n",
    "            response_model=response_model,\n",
    "            **kwargs\n",
    "        )\n",
    "        return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e0df6",
   "metadata": {},
   "source": [
    "# Initialize the LLMCaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0e740ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = LLMCaller(\n",
    "    api_key=os.getenv(\"WX_API_KEY\"),\n",
    "    project_id=os.getenv(\"WX_PROJECT_ID_RAG\"),\n",
    "    api_url=os.getenv(\"WX_URL\"),\n",
    "    model_id=\"watsonx/mistralai/mistral-large\",\n",
    "    params={\"max_tokens\": 100}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986349dd",
   "metadata": {},
   "source": [
    "## Define the EmotionResponse Model\n",
    "\n",
    "This Pydantic model specifies the expected output format from the LLM when detecting emotions in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3a9c3d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionResponse(BaseModel):\n",
    "    emotions: list[str] = Field(..., description=\"The list of Ekman emotions expressed in the review.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680f3df",
   "metadata": {},
   "source": [
    "## Test the Emotion Detection\n",
    "\n",
    "This example sends a sample input to the `LLMCaller` using the `EmotionResponse` model. \n",
    "The input text contains mixed emotional signals, and the model is expected to return \n",
    "a list of all applicable Ekman emotions (e.g., both \"fear\" and \"joy\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "87680c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmotionResponse(emotions=['fear', 'joy'])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"that scared me! that was a lot of fun\", response_model=EmotionResponse) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f02fc",
   "metadata": {},
   "source": [
    "# Manual Prompt and Direct LLM Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c6f285d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a prompt\n",
    "prompt = \"\"\"You are an emotion detection expert. Identify one or more of the following 7 emotions sadness, anger, joy, surprise, fear, disgust, neutral. Do not make new emotions.\n",
    "\"\"\"\n",
    "\n",
    "# make a request to the LLM\n",
    "response = client.chat.completions.create( \n",
    "            model=\"watsonx/mistralai/mistral-large\", \n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt, \n",
    "                }\n",
    "            ],\n",
    "            project_id=os.getenv(\"WX_PROJECT_ID_RAG\"), \n",
    "            apikey=os.getenv(\"WX_API_KEY\"),\n",
    "            api_base=os.getenv(\"WX_API_URL\"),\n",
    "            response_model=EmotionResponse, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8799611b",
   "metadata": {},
   "source": [
    "# Upload ekman_test_with_predictions_sample as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only the first N rows for faster testing\n",
    "N = 50\n",
    "df_test = pd.read_csv(\"data/ekman_test.csv\").head(N)\n",
    "\n",
    "\n",
    "predicted_emotions_list = []\n",
    "\n",
    "for text in df_test[\"text\"]:\n",
    "# Call the LLM to predict emotion using the EmotionPrediction response model\n",
    "    try:\n",
    "        response = llm.invoke(prompt=text, response_model=EmotionPrediction)\n",
    "        predicted_emotions_list.append([response.emotion])\n",
    "\n",
    "# Handle errors gracefully, defaulting to \"neutral\" if prediction fails\n",
    "    except Exception as e:\n",
    "        print(f\"Error with: {text[:50]}... -> {e}\")\n",
    "        predicted_emotions_list.append([\"neutral\"])\n",
    "\n",
    "\n",
    "\n",
    "df_test[\"predicted_emotions\"] = [\", \".join(e) for e in predicted_emotions_list]\n",
    "df_test.to_csv(\"data/ekman_test_with_predictions_sample.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cceb34c",
   "metadata": {},
   "source": [
    "# Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.41      0.39      0.40        74\n",
      "     Disgust       0.06      0.33      0.10         9\n",
      "        Fear       0.38      0.56      0.45         9\n",
      "         Joy       0.69      0.50      0.58       199\n",
      "     Neutral       0.44      0.36      0.40       140\n",
      "     Sadness       0.18      0.41      0.26        29\n",
      "    Surprise       0.26      0.28      0.27        40\n",
      "\n",
      "    accuracy                           0.42       500\n",
      "   macro avg       0.35      0.40      0.35       500\n",
      "weighted avg       0.50      0.42      0.45       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load CSV\n",
    "df = pd.read_csv('data/ekman_test_with_predictions_sample.csv')\n",
    "\n",
    "#Define your one-hot true-emotion columns\n",
    "emotion_cols = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "#Ensure these columns actually exist\n",
    "missing = [c for c in emotion_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"The following emotion columns are missing: {missing}\")\n",
    "\n",
    "#Derive the true label from the one-hot columns\n",
    "df['actual'] = df[emotion_cols].idxmax(axis=1).str.capitalize()\n",
    "\n",
    "#Standardize predicted labels\n",
    "df['predicted'] = df['predicted_emotions'].str.capitalize()\n",
    "\n",
    "print(classification_report(\n",
    "    df['actual'],\n",
    "    df['predicted'],\n",
    "    zero_division=0\n",
    "))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
