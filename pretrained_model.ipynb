{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5634f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import TypeVar, Any\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from instructor import from_litellm, Mode\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57b264",
   "metadata": {},
   "source": [
    "# Load definitions from the Oxford dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/definitions_ekman_emotions.csv\")\n",
    "definitions = \"\\n\".join(f\"{row['emotion']}: {row['definition']}\" for _, row in df.iterrows())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35149508",
   "metadata": {},
   "source": [
    "# Create a system prompt for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c21459",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You’re given a piece of text.  Your job is to pick **all** emotions (one or more) from this list:\n",
    "  [joy, anger, sadness, fear, disgust, surprise, neutral]\n",
    "\n",
    "Respond with a **JSON array** of exact emotion keywords.  \n",
    "– If more than one emotion fits, list them all.  \n",
    "– If none apply, return an empty array: []  \n",
    "– Don’t include any explanation or extra text.\n",
    "\n",
    "Emotion definitions:\n",
    "{definitions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85246fc1",
   "metadata": {},
   "source": [
    "# Creating instructor client from litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44267459",
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.drop_params = True\n",
    "client = from_litellm(completion, mode=Mode.JSON)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09011bdd",
   "metadata": {},
   "source": [
    "# Define EkmanEmotion as a Literal of valid emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fdbbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EkmanEmotion = Literal[\"Anger\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\", \"Surprise\", \"Neutral\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f904887",
   "metadata": {},
   "source": [
    "# Define the EmotionPrediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c9956d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class EmotionPrediction(BaseModel):\n",
    "    emotion: List[EkmanEmotion]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a1e09",
   "metadata": {},
   "source": [
    "# Initialize the LLM client with JSON response mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7295720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a base response model using Pydantic.\n",
    "class BaseResponse(BaseModel):\n",
    "    \"\"\"A default response model that stores a list of predicted Ekman emotions. We will use this to predict the emotions of a review.\"\"\"\n",
    "    answer: str\n",
    "\n",
    "# Define a generic type for later use, bounded to Pydantic BaseModel\n",
    "ResponseType = TypeVar(\"ResponseType\", bound=BaseModel)\n",
    "\n",
    "class LLMCaller:\n",
    "    \"\"\"\n",
    "    A class to interact with a Large Language Model (LLM)\n",
    "    using the LiteLLM and Instructor libraries.\n",
    "    \n",
    "    Designed to send prompts and receive structured responses\n",
    "    as Pydantic models (e.g., predicted emotions).\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key: str, project_id: str, api_url: str, model_id: str, params: dict[str, Any]):\n",
    "        \"\"\"Initializes the LLMCaller with Watsonx credentials and configuration.\"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.project_id = project_id\n",
    "        self.api_url = api_url\n",
    "        self.model_id = model_id\n",
    "        self.params = params\n",
    "\n",
    "        litellm.drop_params = True\n",
    "        self.client = from_litellm(completion, mode=Mode.JSON)\n",
    "\n",
    "    def create_response_model(self, title: str, fields: dict) -> ResponseType:\n",
    "        \"\"\" Dynamically creates a Pydantic response model for the LLM's output.\n",
    "        Args:\n",
    "            title (str): The name of the response model.\n",
    "            fields (dict): A dictionary defining the fields of the response model.\n",
    "                           Keys are field names, and values are tuples of (type, Field).\n",
    "\n",
    "        Returns:\n",
    "            ResponseType: A dynamically created Pydantic model class.\n",
    "        \"\"\"\n",
    "        return create_model(title, **fields, __base__=BaseResponse)\n",
    "\n",
    "    def invoke(self, prompt: str, response_model: ResponseType = BaseResponse, **kwargs) -> ResponseType:\n",
    "        \"\"\" Sends a prompt to the LLM and retrieves a structured response.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt to send to the LLM.\n",
    "            response_model (ResponseType): The Pydantic model to structure the LLM's response.\n",
    "                                           Defaults to BaseResponse.\n",
    "            **kwargs: Additional arguments to pass to the LLM client.\n",
    "\n",
    "        Returns:\n",
    "            ResponseType: The structured response from the LLM, parsed into the specified response model.\n",
    "        \"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt + \"\\n\\nRespond using this structure: \" + str(response_model.__annotations__)\n",
    "            }],\n",
    "            project_id=self.project_id,\n",
    "            apikey=self.api_key,\n",
    "            api_base=self.api_url,\n",
    "            response_model=response_model,\n",
    "            **kwargs\n",
    "        )\n",
    "        return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e0df6",
   "metadata": {},
   "source": [
    "# Initialize the LLMCaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e740ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = LLMCaller(\n",
    "    api_key=os.getenv(\"WX_API_KEY\"),\n",
    "    project_id=os.getenv(\"WX_PROJECT_ID_RAG\"),\n",
    "    api_url=os.getenv(\"WX_URL\"),\n",
    "    model_id=\"watsonx/mistralai/mistral-large\",\n",
    "    params={\"max_tokens\": 100}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986349dd",
   "metadata": {},
   "source": [
    "## Define the EmotionResponse Model\n",
    "\n",
    "This Pydantic model specifies the expected output format from the LLM when detecting emotions in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a9c3d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionResponse(BaseModel):\n",
    "    emotions: list[str] = Field(..., description=\"The list of Ekman emotions expressed in the review.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680f3df",
   "metadata": {},
   "source": [
    "## Test the Emotion Detection\n",
    "\n",
    "This example sends a sample input to the `LLMCaller` using the `EmotionResponse` model. \n",
    "The input text contains mixed emotional signals, and the model is expected to return \n",
    "a list of all applicable Ekman emotions (e.g., both \"fear\" and \"joy\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87680c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmotionResponse(emotions=['fear', 'joy'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"that scared me! that was a lot of fun\", response_model=EmotionResponse) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f02fc",
   "metadata": {},
   "source": [
    "# Manual Prompt and Direct LLM Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f285d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a prompt\n",
    "prompt = \"\"\"You are a highly precise emotion-classification agent that excels at detecting emotion from text. \n",
    "you will be given a text / review and similar examples with labels, along with the definitions of each the emotions.\n",
    "\n",
    "Your task is to read a text / customer review, understand its content, and assign *one or more* of these emotions: \n",
    "anger, disgust, fear, joy, sadness, surprise, or neutral if no clear emotion is detected.\n",
    "\"\"\"\n",
    "\n",
    "# make a request to the LLM\n",
    "response = client.chat.completions.create( \n",
    "            model=\"watsonx/mistralai/mistral-large\", \n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt, \n",
    "                }\n",
    "            ],\n",
    "            project_id=os.getenv(\"WX_PROJECT_ID_RAG\"), \n",
    "            apikey=os.getenv(\"WX_API_KEY\"),\n",
    "            api_base=os.getenv(\"WX_API_URL\"),\n",
    "            response_model=EmotionResponse, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8799611b",
   "metadata": {},
   "source": [
    "# Make predictions and upload ekman_test_with_predictions_sample as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26e815cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['Joy'] (<class 'list'>)\n",
      "1: ['Joy'] (<class 'list'>)\n",
      "2: ['Disgust'] (<class 'list'>)\n",
      "3: ['Neutral'] (<class 'list'>)\n",
      "4: ['Anger', 'Disgust', 'Sadness'] (<class 'list'>)\n"
     ]
    }
   ],
   "source": [
    "# Load only the first N rows for faster testing\n",
    "N = 500\n",
    "df_test = pd.read_csv(\"data/ekman_test.csv\").head(N)\n",
    "\n",
    "predicted_emotions_list = []\n",
    "\n",
    "# Use the LLMCaller instance to predict emotions\n",
    "for text in df_test[\"text\"]:\n",
    "    try:\n",
    "        response = llm.invoke(prompt=text, response_model=EmotionPrediction)\n",
    "        predicted_emotions_list.append(response.emotion)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with: {text[:50]}... -> {e}\")\n",
    "        predicted_emotions_list.append([\"neutral\"])  # fallback\n",
    "\n",
    "# Print some examples for inspection\n",
    "for i, item in enumerate(predicted_emotions_list[:5]):\n",
    "    print(f\"{i}: {item} ({type(item)})\")\n",
    "\n",
    "# Clean and assign predictions\n",
    "cleaned_predictions = []\n",
    "for e in predicted_emotions_list:\n",
    "    if isinstance(e, list):\n",
    "        cleaned_predictions.append(\", \".join(str(x) for x in e))\n",
    "    else:\n",
    "        cleaned_predictions.append(str(e))\n",
    "\n",
    "df_test[\"predicted_emotions\"] = cleaned_predictions\n",
    "\n",
    "# Save to CSV\n",
    "df_test.to_csv(\"data/ekman_test_with_predictions_sample.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4fab8d",
   "metadata": {},
   "source": [
    "# Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfde9b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.36      0.53      0.43        74\n",
      "     Disgust       0.08      0.57      0.14        14\n",
      "        Fear       0.13      0.67      0.22         9\n",
      "         Joy       0.69      0.59      0.64       207\n",
      "     Neutral       0.41      0.39      0.40       140\n",
      "     Sadness       0.20      0.49      0.28        45\n",
      "    Surprise       0.23      0.39      0.29        64\n",
      "\n",
      "   micro avg       0.35      0.50      0.41       553\n",
      "   macro avg       0.30      0.52      0.34       553\n",
      "weighted avg       0.46      0.50      0.46       553\n",
      " samples avg       0.41      0.51      0.44       553\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcuskrarup/anaconda3/envs/tf-env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# True labels as lists of emotions\n",
    "y_true = df_test[[\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]].values\n",
    "y_true = y_true.tolist()  # already binary format\n",
    "\n",
    "# Predicted emotions as string -> list\n",
    "df_test[\"predicted_emotions\"] = df_test[\"predicted_emotions\"].fillna(\"\").apply(lambda x: [e.strip().capitalize() for e in x.split(\",\") if e.strip()])\n",
    "\n",
    "# Binarize predicted labels\n",
    "mlb = MultiLabelBinarizer(classes=[\"Anger\", \"Disgust\", \"Fear\", \"Joy\", \"Neutral\", \"Sadness\", \"Surprise\"])\n",
    "y_pred = mlb.fit_transform(df_test[\"predicted_emotions\"])\n",
    "\n",
    "#classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=mlb.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35838f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28\n"
     ]
    }
   ],
   "source": [
    "#Print accuracy from the classification report\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
