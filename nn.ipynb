{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e0110f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69005660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marcuskrarup/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018f412",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1e01561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Ekman emotions to DataFrame\n",
    "df_train_ekman = pd.read_csv('data/ekman_train.csv')\n",
    "df_val_ekman = pd.read_csv('data/ekman_val.csv')\n",
    "df_test_ekman = pd.read_csv('data/ekman_test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb04edd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Counts:\n",
      " anger        4824\n",
      "disgust       818\n",
      "fear          695\n",
      "joy         13877\n",
      "neutral      9290\n",
      "sadness      2967\n",
      "surprise     4783\n",
      "dtype: int64\n",
      "\n",
      "Test Counts:\n",
      " anger       1635\n",
      "disgust      281\n",
      "fear         257\n",
      "joy         4622\n",
      "neutral     3046\n",
      "sadness     1042\n",
      "surprise    1638\n",
      "dtype: int64\n",
      "\n",
      "Validation Counts:\n",
      " anger       1568\n",
      "disgust      250\n",
      "fear         258\n",
      "joy         4599\n",
      "neutral     3152\n",
      "sadness      977\n",
      "surprise    1636\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences for each emotion in train, test, and val datasets\n",
    "train_counts = df_train_ekman.iloc[:, 1:].sum()\n",
    "test_counts = df_test_ekman.iloc[:, 1:].sum()\n",
    "val_counts = df_val_ekman.iloc[:, 1:].sum()\n",
    "\n",
    "print(\"Train Counts:\\n\", train_counts)\n",
    "print(\"\\nTest Counts:\\n\", test_counts)\n",
    "print(\"\\nValidation Counts:\\n\", val_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d301166",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ff79a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1449c",
   "metadata": {},
   "source": [
    "# Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbe94e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = df_train_ekman['text'].apply(preprocess)\n",
    "word_counts = Counter(token for tokens in tokenized_texts for token in tokens)\n",
    "vocab = {word: i + 2 for i, (word, _) in enumerate(word_counts.items())}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = 1\n",
    "\n",
    "def encode(text):\n",
    "    return [vocab.get(t, vocab['<UNK>']) for t in preprocess(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1567cc",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "219b2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.X = [torch.tensor(encode(text)) for text in df['text']]\n",
    "        self.y = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    return padded_seqs, torch.stack(labels)\n",
    "\n",
    "train_loader = DataLoader(EmotionDataset(df_train_ekman), batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(EmotionDataset(df_val_ekman), batch_size=32, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(EmotionDataset(df_test_ekman), batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0ae05",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9983cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec3a7b",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35b1a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EmotionLSTM(len(vocab), embed_dim=100, hidden_dim=128, output_dim=7).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd1d9f8",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bb01046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 402.1999\n",
      "Epoch 2, Loss: 371.5362\n",
      "Epoch 3, Loss: 335.3612\n",
      "Epoch 4, Loss: 311.0528\n",
      "Epoch 5, Loss: 285.8978\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141fdd32",
   "metadata": {},
   "source": [
    "# Evaluation (Validation or Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a09ffecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 258.3643\n",
      "Epoch 2, Loss: 228.1770\n",
      "Epoch 3, Loss: 198.7505\n",
      "Epoch 4, Loss: 172.0502\n",
      "Epoch 5, Loss: 147.9996\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed103e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            preds = model(X_batch)\n",
    "            preds = (preds > 0.5).float()\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += torch.numel(y_batch)\n",
    "    print(f\"Accuracy: {correct / total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f226b778",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate(\n\u001b[1;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m      3\u001b[0m     data_loader \u001b[38;5;241m=\u001b[39m val_loader,  \n\u001b[1;32m      4\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m criterion,\n\u001b[1;32m      5\u001b[0m     device \u001b[38;5;241m=\u001b[39m device,\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "evaluate(\n",
    "    model = model.to(device),\n",
    "    data_loader = val_loader,  \n",
    "    criterion = criterion,\n",
    "    device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b85ba6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:\n",
      "Accuracy: 0.8517\n"
     ]
    }
   ],
   "source": [
    "print(\"Test:\")\n",
    "evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4703f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden_dim=64, batch_size=16, epochs=3\n",
      "Validation Loss: 0.3142\n",
      "Training with hidden_dim=64, batch_size=16, epochs=5\n",
      "Validation Loss: 0.3197\n",
      "Training with hidden_dim=64, batch_size=32, epochs=3\n",
      "Validation Loss: 0.3216\n",
      "Training with hidden_dim=64, batch_size=32, epochs=5\n",
      "Validation Loss: 0.3185\n",
      "Training with hidden_dim=128, batch_size=16, epochs=3\n",
      "Validation Loss: 0.3082\n",
      "Training with hidden_dim=128, batch_size=16, epochs=5\n",
      "Validation Loss: 0.3181\n",
      "Training with hidden_dim=128, batch_size=32, epochs=3\n",
      "Validation Loss: 0.3125\n",
      "Training with hidden_dim=128, batch_size=32, epochs=5\n",
      "Validation Loss: 0.3153\n",
      "\n",
      "✅ Best Configuration:\n",
      "Hidden Dim: 128, Batch Size: 16, Epochs: 3\n",
      "Validation Loss: 0.3082\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Grid search hyperparameters\n",
    "EPOCHS_LIST = [3, 5]\n",
    "HIDDEN_DIMS = [64, 128]\n",
    "BATCH_SIZES = [16, 32]\n",
    "\n",
    "best_config = None\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "def run_training(hidden_dim, batch_size, epochs):\n",
    "    # Create loaders with current batch size\n",
    "    train_loader = DataLoader(EmotionDataset(df_train_ekman), batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(EmotionDataset(df_val_ekman), batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    # Model\n",
    "    model = EmotionLSTM(len(vocab), embed_dim=100, hidden_dim=hidden_dim, output_dim=7).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "# Grid search loop\n",
    "for hidden_dim, batch_size, epochs in product(HIDDEN_DIMS, BATCH_SIZES, EPOCHS_LIST):\n",
    "    print(f\"Training with hidden_dim={hidden_dim}, batch_size={batch_size}, epochs={epochs}\")\n",
    "    val_loss = run_training(hidden_dim, batch_size, epochs)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_config = (hidden_dim, batch_size, epochs)\n",
    "\n",
    "print(\"\\n✅ Best Configuration:\")\n",
    "print(f\"Hidden Dim: {best_config[0]}, Batch Size: {best_config[1]}, Epochs: {best_config[2]}\")\n",
    "print(f\"Validation Loss: {best_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
