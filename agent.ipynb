{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c1dd17b",
   "metadata": {},
   "source": [
    "## Building a router agent to classify text/reviews and draft up a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b2f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in libraries\n",
    "import os\n",
    "os.environ['PYTHONUTF8'] = '1'\n",
    "from typing import TypeVar, Any\n",
    "\n",
    "# litellm libraries\n",
    "import litellm\n",
    "from litellm.types.utils import ModelResponse, Message\n",
    "from litellm import completion\n",
    "from instructor import from_litellm, Mode\n",
    "\n",
    "# misc libraries\n",
    "from pydantic import BaseModel, create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in libraries\n",
    "from typing import Literal, TypedDict,  Any, Optional, Tuple, List, Dict, Union\n",
    "\n",
    "# langgraph libraries\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.runnables.graph import  MermaidDrawMethod\n",
    "\n",
    "# misc libraries\n",
    "from pydantic import Field\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from decouple import config\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# local modules\n",
    "from src.llm import LLMCaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Valdemar Schultz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Valdemar\n",
      "[nltk_data]     Schultz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import List, Dict\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2f51dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7acfb8",
   "metadata": {},
   "source": [
    "Watsonx credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e061463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WX_API_KEY = os.getenv(\"WX_API_KEY\")\n",
    "WX_PROJECT_ID_RAG = os.getenv(\"WX_PROJECT_ID_RAG\")\n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fddbe",
   "metadata": {},
   "source": [
    "Getting our LLM caller class - Defined in LLM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c67fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LLMCaller(\n",
    "    api_key=WX_API_KEY,\n",
    "    project_id=WX_PROJECT_ID_RAG,\n",
    "    api_url=WX_API_URL,\n",
    "    model_id=\"watsonx/mistralai/mistral-large\",\n",
    "    params={\n",
    "        GenParams.TEMPERATURE: 0.3,\n",
    "        GenParams.MAX_NEW_TOKENS: 50,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2e484",
   "metadata": {},
   "source": [
    "### Creating our few shot examples data set for the LLM to learn from\n",
    "We will use the few shot examples to train the LLM to classify the text and draft a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a175c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load your CSV\n",
    "csv_path = os.path.join(\"data\", \"ekman_train.csv\")\n",
    "# Expect columns: text, ekman_emotion\n",
    "df = pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3a89b",
   "metadata": {},
   "source": [
    "creating few shot dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43cea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed DataFrame head:\n",
      "                                                text ekman_emotion\n",
      "0                        [NAME] good one i like that           joy\n",
      "1                  That’s actually interesting to me           joy\n",
      "2  Why is this getting downvoted. I love Meepo so...       sadness\n",
      "3                    I'm not offended. Just curious.      surprise\n",
      "4  We have reached the stage where below 2 millio...       neutral\n"
     ]
    }
   ],
   "source": [
    "# Define the order of emotion columns. idxmax will pick the first one if multiple emotions are present.\n",
    "emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# Find the emotion for each row.\n",
    "# This creates a new series where each value is the column name of the first '1' found in emotion_columns.\n",
    "df['ekman_emotion'] = df[emotion_columns].idxmax(axis=1)\n",
    "\n",
    "# Recreate the DataFrame with only the 'text' and 'ekman_emotion' columns.\n",
    "# This overwrites the original df, which is expected by the subsequent cells.\n",
    "df = df[['text', 'ekman_emotion']]\n",
    "\n",
    "# You can print the head of the modified DataFrame to verify\n",
    "print(\"Transformed DataFrame head:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbd0b3",
   "metadata": {},
   "source": [
    "Createing my embedding tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9eaf922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load a local embedding model\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3. Helper to embed one or more texts\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    texts: a single string or list of strings\n",
    "    returns: numpy array of shape (n_texts, 384)\n",
    "    \"\"\"\n",
    "    # If a single string, wrap it in a list for the model call\n",
    "    single = False\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "        single = True\n",
    "\n",
    "    embs = embed_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "    # If only one input, return its vector directly\n",
    "    return embs[0] if single else embs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ecd82c",
   "metadata": {},
   "source": [
    "Chunking the few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e1a6c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 1400 few-shot examples (7 emotions × up to 200 each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Chunk long texts into 1–2 sentence chunks (if you want)\n",
    "def chunk_text(text, max_sentences=2):\n",
    "    sents = sent_tokenize(text)\n",
    "    for i in range(0, len(sents), max_sentences):\n",
    "        yield \" \".join(sents[i : i + max_sentences]).strip()\n",
    "\n",
    "shots = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row[\"text\"].strip()\n",
    "    emo  = row[\"ekman_emotion\"]\n",
    "    for chunk in chunk_text(text):\n",
    "        shots.append({\"text\": chunk, \"emotion\": emo})\n",
    "\n",
    "# 4) Balance to N examples per emotion (to avoid over-representing any one)\n",
    "N = 200\n",
    "by_emo = defaultdict(list)\n",
    "for shot in shots:\n",
    "    by_emo[shot[\"emotion\"]].append(shot)\n",
    "\n",
    "few_shots = []\n",
    "for emo, lst in by_emo.items():\n",
    "    few_shots.extend(lst[:N])\n",
    "\n",
    "print(f\"Prepared {len(few_shots)} few-shot examples \"\n",
    "      f\"({len(by_emo)} emotions × up to {N} each)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aff0f8",
   "metadata": {},
   "source": [
    "Embedding the few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9e393c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 1400 few-shot examples into a (1400, 384) array\n"
     ]
    }
   ],
   "source": [
    "# 1) Embed each shot in place\n",
    "for shot in few_shots:\n",
    "    shot[\"embedding\"] = embed(shot[\"text\"])\n",
    "\n",
    "# 2) (Optional) Build an (N, D) matrix for retrieval\n",
    "emb_matrix = np.vstack([shot[\"embedding\"] for shot in few_shots]).astype(\"float32\")\n",
    "\n",
    "print(f\"Embedded {len(few_shots)} few-shot examples into a {emb_matrix.shape} array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfa13cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your state type\n",
    "class ReviewState(TypedDict):\n",
    "    review_text: str\n",
    "    few_shot_examples: List[Dict[str, str]] \n",
    "    emotion: Optional[str]\n",
    "    draft_reply: Optional[str]\n",
    "    verbose: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038cd1a",
   "metadata": {},
   "source": [
    "Creating the nodes of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "483cc78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shots: List[Dict[str, object]] = few_shots # List of few-shot examples\n",
    "emb_matrix: np.ndarray = np.vstack([shot[\"embedding\"] for shot in few_shots]).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_review(state: ReviewState) -> dict:\n",
    "    \"\"\"Reviewbot loads and logs the incoming customer review\"\"\"\n",
    "    review = state[\"review_text\"]\n",
    "    if state[\"verbose\"]:\n",
    "        print(f\"Reviewbot received review: “{review}”\")\n",
    "    # no changes to state here\n",
    "    return {}\n",
    "\n",
    "#dynamic few-shotting technique, matches unseen reviews to the most similar few-shot examples\n",
    "def retrieve_few_shot_examples(state: ReviewState) -> dict:\n",
    "    \"\"\"\n",
    "    Embed the incoming review, compute cosine similarity against your pre-embedded few_shots,\n",
    "    and stash the top-5 (text, emotion) pairs into state['few_shot_examples'].\n",
    "    \"\"\"\n",
    "    # 1) Embed the new review\n",
    "    q_emb = embed(state[\"review_text\"]).reshape(1, -1).astype(\"float32\")\n",
    "    \n",
    "    # 2) Compute cosine similarities\n",
    "    sims = cosine_similarity(q_emb, emb_matrix)[0]  # shape (N,)\n",
    "    \n",
    "    # 3) Grab top-5 indices\n",
    "    topk = sims.argsort()[-5:][::-1]\n",
    "    \n",
    "    new_examples = []\n",
    "    for i in topk:\n",
    "        if not (0 <= i < len(few_shots)):\n",
    "            # Optionally, log a warning here if an index is out of bounds,\n",
    "            # though with argsort this shouldn't happen if emb_matrix and few_shots are aligned.\n",
    "            # print(f\"Warning: Index i={i} is out of bounds for few_shots (len={len(few_shots)})\")\n",
    "            continue\n",
    "\n",
    "        current_shot_element = few_shots[i]\n",
    "\n",
    "        if not isinstance(current_shot_element, dict):\n",
    "            # Optionally, log an error or warning here\n",
    "            # print(f\"Warning: few_shots[{i}] is not a dictionary! It is a {type(current_shot_element)}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            text_val = current_shot_element[\"text\"]\n",
    "            emotion_val = current_shot_element[\"emotion\"]\n",
    "            \n",
    "            example_to_add = {\"text\": text_val, \"emotion\": emotion_val}\n",
    "            new_examples.append(example_to_add)\n",
    "\n",
    "        except KeyError as e:\n",
    "            # Optionally, log this error if it's critical\n",
    "            # print(f\"Error: KeyError occurred while processing: {current_shot_element}. Key: {e.args[0]}\")\n",
    "            # Depending on desired robustness, you might skip this example or re-raise\n",
    "            continue # Skip this example if a key is missing\n",
    "        except Exception as e:\n",
    "            # Optionally, log other unexpected errors\n",
    "            # print(f\"Error: An unexpected error occurred while processing: {current_shot_element}. Error: {e}\")\n",
    "            continue # Skip this example on other errors\n",
    "            \n",
    "    return {\"few_shot_examples\": new_examples}\n",
    "\n",
    "def classify_emotion(state: ReviewState) -> dict:\n",
    "    \"\"\"Use the LLM with few-shot context + Ekman definitions to pick emotions.\"\"\"\n",
    "    text = state[\"review_text\"]\n",
    "    # Format few-shot context\n",
    "    example_lines = \"\\n\".join(\n",
    "        f\"- “{ex['text']}” → {ex['emotion']}\"\n",
    "        for ex in state[\"few_shot_examples\"]\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a highly precise expert emotion-classification agent. Your task is to read a text / customer review  and assign **one or more** of these labels: \n",
    "anger, disgust, fear, joy, sadness, surprise, or neutral.  \n",
    "\n",
    "Here are a few similar, examples **With Labels** to help you classify the new review:\n",
    "{example_lines}\n",
    "\n",
    "Now classify this new review into one or more of:\n",
    "Anger, disgust, fear, joy, sadness, surprise or neutral.\n",
    "\n",
    "Review:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "Respond **only** in JSON with:\n",
    "  • emotion: a string or list of strings from the set above.\n",
    "\"\"\"\n",
    "    response_model = model.create_response_model(\n",
    "        \"FewShotEmotionClassification\",\n",
    "        {\n",
    "            \"emotion\": (\n",
    "                # allow multi-label if you want, or just one\n",
    "                List[Literal[\"anger\",\"disgust\",\"fear\",\"joy\",\"sadness\",\"surprise\",\"neutral\"]],\n",
    "                Field(description=\"The chosen Ekman emotion(s)\")\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    resp = model.invoke(prompt, response_model=response_model)\n",
    "    \n",
    "    return {\"emotion\": resp.emotion}\n",
    "\n",
    "def draft_response(state: ReviewState) -> dict:\n",
    "    \"\"\"Draft an empathetic reply based on the classified emotion\"\"\"\n",
    "    text = state[\"review_text\"]\n",
    "    emo  = state[\"emotion\"]\n",
    "    prompt = f\"\"\"A customer expresses **{emo}** in this review:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "You are a senior customer-service specialist with excellent empathy and brand voice consistency.  \n",
    "Given a review and its classified emotion(s), draft a short reply that:\n",
    "\n",
    "  - Acknowledges their {emo}, WITHOUT writing the specific emotions in the response,\n",
    "  - Addresses any concerns they raise,\n",
    "  - Invites further dialogue if needed.\n",
    "\"\"\"\n",
    "    resp = model.invoke(prompt)\n",
    "    return {\"draft_reply\": resp.answer}\n",
    "\n",
    "\n",
    "def notify_user(state: ReviewState) -> dict:\n",
    "    \"\"\"Present the final draft back to the support employee\"\"\"\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\"review : {state[\"review_text\"]}\")\n",
    "        print(f\"Review classified as: {state['emotion']}\")\n",
    "        print(\"Response:\")\n",
    "        print(state[\"draft_reply\"])\n",
    "        print(\"=\"*40 + \"\\n\")\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f84eb",
   "metadata": {},
   "source": [
    "No need for conditional edge as it is a linear system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bddc11",
   "metadata": {},
   "source": [
    "### Creating Graph of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "050e574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "review_graph = StateGraph(ReviewState)  # Initialize with your ReviewState\n",
    "\n",
    "# Add nodes\n",
    "review_graph.add_node(\"read review\",       read_review)\n",
    "#review_graph.add_node(\"fetch emotion definitions\", fetch_emotion_definitions)\n",
    "review_graph.add_node(\"retrieve few-shot examples\", retrieve_few_shot_examples)\n",
    "review_graph.add_node(\"classify emotion\",  classify_emotion)\n",
    "review_graph.add_node(\"draft response\",    draft_response)\n",
    "review_graph.add_node(\"notify user\",      notify_user)\n",
    "\n",
    "# Wire up the edges\n",
    "# 1) START → read_review\n",
    "review_graph.add_edge(START, \"read review\")\n",
    "\n",
    "# 2) read_review → fetch_emotion_definitions\n",
    "review_graph.add_edge(\"read review\", \"retrieve few-shot examples\")\n",
    "\n",
    "review_graph.add_edge(\"retrieve few-shot examples\", \"classify emotion\")\n",
    "# 2) retrieve_few_shot_examples → classify_emotion\n",
    "\n",
    "# 3) classify_emotion → route_review\n",
    "review_graph.add_edge(\"classify emotion\", \"draft response\")\n",
    "\n",
    "#4 ) draft_response → notify_user\n",
    "review_graph.add_edge(\"draft response\", \"notify user\")\n",
    "\n",
    "# 5) notify_user → END\n",
    "review_graph.add_edge(\"notify user\", END)\n",
    "\n",
    "\n",
    "compiled_graph = review_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f40cdd",
   "metadata": {},
   "source": [
    "Print the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(compiled_graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0e29d",
   "metadata": {},
   "source": [
    "Testing the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d1470e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate test texts/ reviews to pass to my review bot\n",
    "Example_text1 = \"I love this product! It works great and exceeded my expectations.\"\n",
    "Example_text2 = \"I am very disappointed with the service. It was not what I expected.\"\n",
    "Example_text3 = \"The product is okay, but it could be better. I expected more.\"\n",
    "Example_text4 = \"the service was nothing special. I was hoping for a better experience. but product was good.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e4e79",
   "metadata": {},
   "source": [
    "System is correctly categorizing the text and drafting a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fe083d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing review\n",
      "Reviewbot received review: “the service was nothing special. I was hoping for a better experience. but product was good.”\n",
      "\n",
      "========================================\n",
      "Review classified as: ['neutral']\n",
      "Response:\n",
      "Thank you for your feedback. We're sorry to hear that your experience with our service didn't meet your expectations, but we're glad you found the product to be good. We strive to improve, so if there's anything specific you'd like us to address, please let us know.\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process the legitimate email\n",
    "print(\"\\nProcessing review\")\n",
    "test_examples = compiled_graph.invoke(\n",
    "    {\n",
    "        \"review_text\": Example_text4, # invoke the graph with the email text\n",
    "        \"verbose\": True,  # enable verbose output\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d864d",
   "metadata": {},
   "source": [
    "Evaluating the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d23961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emotion columns (ensure this matches your dataset and classify_emotion output)\n",
    "emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# 1. Load test data\n",
    "try:\n",
    "    df_test = pd.read_csv(\"data/ekman_test.csv\")\n",
    "    df_test = df_test[:60]#small dataset for testing the classification function\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: data/ekman_test.csv not found. Make sure the path is correct.\")\n",
    "    raise\n",
    "\n",
    "# Prepare lists for true and predicted labels\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aa0a3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating classify_emotion function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test data:  25%|██▌       | 15/60 [00:59<03:50,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during classify_emotion for text: 'Thats fair. I know its a joke it just always struc...': 1 validation error for FewShotEmotionClassification\n",
      "emotion\n",
      "  Field required [type=missing, input_value={'answer': 'Thats fair. I...te it off. No worries.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test data: 100%|██████████| 60/60 [03:19<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for classify_emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.17      0.17      0.17         6\n",
      "     disgust       0.00      0.00      0.00         1\n",
      "        fear       0.33      0.50      0.40         2\n",
      "         joy       0.72      0.54      0.62        24\n",
      "     neutral       0.43      0.60      0.50        20\n",
      "     sadness       0.57      0.57      0.57         7\n",
      "    surprise       0.29      0.57      0.38         7\n",
      "\n",
      "   micro avg       0.42      0.52      0.47        67\n",
      "   macro avg       0.36      0.42      0.38        67\n",
      "weighted avg       0.50      0.52      0.50        67\n",
      " samples avg       0.43      0.52      0.46        67\n",
      "\n",
      "\n",
      "Overall Micro Averages: Precision: 0.4217, Recall: 0.5224, F1-Score: 0.4667\n",
      "Overall Macro Averages: Precision: 0.3583, Recall: 0.4216, F1-Score: 0.3769\n",
      "Overall Weighted Averages: Precision: 0.5011, Recall: 0.5224, F1-Score: 0.4974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evaluating classify_emotion function...\")\n",
    "# 2. & 3. Get true labels and predictions\n",
    "# Ensure `few_shots`, `emb_matrix` (used by retrieve_few_shot_examples)\n",
    "# and `model` (used by classify_emotion) are initialized from previous cells.\n",
    "\n",
    "for index, row in tqdm(df_test.iterrows(), total=df_test.shape[0], desc=\"Processing test data\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Prepare true labels for this sample\n",
    "    true_emotions_for_sample = [col for col in emotion_columns if col in row and row[col] == 1]\n",
    "    all_true_labels.append(true_emotions_for_sample)\n",
    "    \n",
    "    # Prepare state for classify_emotion\n",
    "    # ReviewState is defined in a previous cell\n",
    "    current_state = ReviewState(\n",
    "        review_text=text,\n",
    "        few_shot_examples=[], \n",
    "        emotion=None,         \n",
    "        draft_reply=None,\n",
    "        verbose=False         \n",
    "    )\n",
    "    # Call retrieve_few_shot_examples\n",
    "    try:\n",
    "        retrieved_state_update = retrieve_few_shot_examples(current_state)\n",
    "        current_state.update(retrieved_state_update)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during retrieve_few_shot_examples for text: '{text[:50]}...': {e}\")\n",
    "        all_pred_labels.append([]) # Append empty list for this sample on error\n",
    "        continue\n",
    "\n",
    "    # Call classify_emotion\n",
    "    try:\n",
    "        classification_result = classify_emotion(current_state)\n",
    "        predicted_emotions_for_sample = classification_result.get('emotion', [])\n",
    "        \n",
    "        if isinstance(predicted_emotions_for_sample, str): # Ensure it's a list\n",
    "            predicted_emotions_for_sample = [predicted_emotions_for_sample]\n",
    "        \n",
    "        all_pred_labels.append(predicted_emotions_for_sample)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during classify_emotion for text: '{text[:50]}...': {e}\")\n",
    "        all_pred_labels.append([]) # Append empty list for this sample on error\n",
    "        continue\n",
    "\n",
    "# 4. Evaluate\n",
    "# Initialize MultiLabelBinarizer with all possible emotion classes\n",
    "mlb = MultiLabelBinarizer(classes=emotion_columns)\n",
    "\n",
    "# Fit on all possible labels to ensure consistent encoding, then transform\n",
    "mlb.fit([emotion_columns]) # Fit with all known classes\n",
    "y_true_binarized = mlb.transform(all_true_labels)\n",
    "y_pred_binarized = mlb.transform(all_pred_labels)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report for classify_emotion:\")\n",
    "report = classification_report(y_true_binarized, y_pred_binarized, target_names=mlb.classes_, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "# Calculate overall metrics\n",
    "precision_micro = precision_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "recall_micro = recall_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "f1_micro = f1_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "\n",
    "precision_macro = precision_score(y_true_binarized, y_pred_binarized, average='macro', zero_division=0)\n",
    "recall_macro = recall_score(y_true_binarized, y_pred_binarized, average='macro', zero_division=0)\n",
    "f1_macro = f1_score(y_true_binarized, y_pred_binarized, average='macro', zero_division=0)\n",
    "\n",
    "precision_weighted = precision_score(y_true_binarized, y_pred_binarized, average='weighted', zero_division=0)\n",
    "recall_weighted = recall_score(y_true_binarized, y_pred_binarized, average='weighted', zero_division=0)\n",
    "f1_weighted = f1_score(y_true_binarized, y_pred_binarized, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nOverall Micro Averages: Precision: {precision_micro:.4f}, Recall: {recall_micro:.4f}, F1-Score: {f1_micro:.4f}\")\n",
    "print(f\"Overall Macro Averages: Precision: {precision_macro:.4f}, Recall: {recall_macro:.4f}, F1-Score: {f1_macro:.4f}\")\n",
    "print(f\"Overall Weighted Averages: Precision: {precision_weighted:.4f}, Recall: {recall_weighted:.4f}, F1-Score: {f1_weighted:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
