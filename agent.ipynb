{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c1dd17b",
   "metadata": {},
   "source": [
    "## Building a router agent to classify text/reviews and draft up a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b2f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in libraries\n",
    "import os\n",
    "os.environ['PYTHONUTF8'] = '1'\n",
    "from typing import TypeVar, Any\n",
    "\n",
    "# litellm libraries\n",
    "import litellm\n",
    "from litellm.types.utils import ModelResponse, Message\n",
    "from litellm import completion\n",
    "from instructor import from_litellm, Mode\n",
    "\n",
    "# misc libraries\n",
    "from pydantic import BaseModel, create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f17f64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in libraries\n",
    "from typing import Literal, TypedDict,  Any, Optional, Tuple, List, Dict, Union\n",
    "\n",
    "# langgraph libraries\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.runnables.graph import  MermaidDrawMethod\n",
    "\n",
    "# misc libraries\n",
    "from pydantic import Field\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from decouple import config\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f231d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# local modules\n",
    "from src.llm import LLMCaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1373d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Valdemar Schultz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import List, Dict\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt') - only needed once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f51dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7acfb8",
   "metadata": {},
   "source": [
    "Watsonx credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e061463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WX_API_KEY = os.getenv(\"WX_API_KEY\")\n",
    "WX_PROJECT_ID_RAG = os.getenv(\"WX_PROJECT_ID_RAG\")\n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fddbe",
   "metadata": {},
   "source": [
    "Getting our LLM caller class - Defined in LLM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c67fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LLMCaller(\n",
    "    api_key=WX_API_KEY,\n",
    "    project_id=WX_PROJECT_ID_RAG,\n",
    "    api_url=WX_API_URL,\n",
    "    model_id=\"watsonx/mistralai/mistral-large\",\n",
    "    params = {  \n",
    "    GenParams.TEMPERATURE: 0.0,\n",
    "    GenParams.MAX_NEW_TOKENS: 50\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2e484",
   "metadata": {},
   "source": [
    "### Creating our few shot examples data set for the LLM to learn from\n",
    "We will use the few shot examples to train the LLM to classify the text and draft a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a175c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load your CSV\n",
    "csv_path = os.path.join(\"data\", \"ekman_train.csv\")\n",
    "# Expect columns: text, ekman_emotion\n",
    "df = pd.read_csv(csv_path)\n",
    "#remove all emojis from the text column\n",
    "df['text'] = df['text'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3a89b",
   "metadata": {},
   "source": [
    "creating few shot dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43cea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed DataFrame head:\n",
      "                                                text ekman_emotion\n",
      "0                        [NAME] good one i like that           joy\n",
      "1                   Thats actually interesting to me           joy\n",
      "2  Why is this getting downvoted. I love Meepo so...       sadness\n",
      "3                    I'm not offended. Just curious.      surprise\n",
      "4  We have reached the stage where below 2 millio...       neutral\n"
     ]
    }
   ],
   "source": [
    "# Define the order of emotion columns. idxmax will pick the first one if multiple emotions are present.\n",
    "emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# Find the emotion for each row.\n",
    "# This creates a new series where each value is the column name of the first '1' found in emotion_columns.\n",
    "df['ekman_emotion'] = df[emotion_columns].idxmax(axis=1)\n",
    "\n",
    "# Recreate the DataFrame with only the 'text' and 'ekman_emotion' columns.\n",
    "# This overwrites the original df, which is expected by the subsequent cells.\n",
    "df = df[['text', 'ekman_emotion']]\n",
    "\n",
    "# You can print the head of the modified DataFrame to verify\n",
    "print(\"Transformed DataFrame head:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbd0b3",
   "metadata": {},
   "source": [
    "Createing my embedding tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9eaf922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load a local embedding model\n",
    "#embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embed_model = SentenceTransformer(\"msmarco-distilbert-base-v4\")\n",
    "\n",
    "\n",
    "# 3. Helper to embed one or more texts\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    texts: a single string or list of strings\n",
    "    returns: numpy array of shape (n_texts,)\n",
    "    \"\"\"\n",
    "    # If a single string, wrap it in a list for the model call\n",
    "    single = False\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "        single = True\n",
    "\n",
    "    embs = embed_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "    # If only one input, return its vector directly\n",
    "    return embs[0] if single else embs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ecd82c",
   "metadata": {},
   "source": [
    "Chunking the few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e1a6c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 5389 few-shot examples (7 emotions × up to 800 each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Chunk long texts into 1–2 sentence chunks (if you want)\n",
    "def chunk_text(text, max_sentences=2):\n",
    "    sents = sent_tokenize(text)\n",
    "    for i in range(0, len(sents), max_sentences):\n",
    "        yield \" \".join(sents[i : i + max_sentences]).strip()\n",
    "\n",
    "shots = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row[\"text\"].strip()\n",
    "    emo  = row[\"ekman_emotion\"]\n",
    "    for chunk in chunk_text(text):\n",
    "        shots.append({\"text\": chunk, \"emotion\": emo})\n",
    "\n",
    "# 4) Balance to N examples per emotion (to avoid over-representing any one)\n",
    "N = 800\n",
    "by_emo = defaultdict(list)\n",
    "for shot in shots:\n",
    "    by_emo[shot[\"emotion\"]].append(shot)\n",
    "\n",
    "few_shots = []\n",
    "for emo, lst in by_emo.items():\n",
    "    few_shots.extend(lst[:N])\n",
    "\n",
    "print(f\"Prepared {len(few_shots)} few-shot examples \"\n",
    "      f\"({len(by_emo)} emotions × up to {N} each)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aff0f8",
   "metadata": {},
   "source": [
    "Embedding the few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9e393c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 5389 few-shot examples into a (5389, 768) array\n"
     ]
    }
   ],
   "source": [
    "# 1) Embed each shot in place\n",
    "for shot in few_shots:\n",
    "    shot[\"embedding\"] = embed(shot[\"text\"])\n",
    "\n",
    "# 2) (Optional) Build an (N, D) matrix for retrieval\n",
    "emb_matrix = np.vstack([shot[\"embedding\"] for shot in few_shots]).astype(\"float32\")\n",
    "\n",
    "print(f\"Embedded {len(few_shots)} few-shot examples into a {emb_matrix.shape} array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfa13cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your state type\n",
    "class ReviewState(TypedDict):\n",
    "    review_text: str\n",
    "    emotion_definitions: List[Dict[str, str]]\n",
    "    few_shot_examples: List[Dict[str, str]] \n",
    "    emotion: Optional[str]\n",
    "    draft_reply: Optional[str]\n",
    "    verbose: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038cd1a",
   "metadata": {},
   "source": [
    "Creating the nodes of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "483cc78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shots: List[Dict[str, object]] = few_shots # List of few-shot examples\n",
    "emb_matrix: np.ndarray = np.vstack([shot[\"embedding\"] for shot in few_shots]).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5f652",
   "metadata": {},
   "source": [
    "defineing the Agent functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_review(state: ReviewState) -> dict:\n",
    "    \"\"\"Reviewbot loads and logs the incoming customer review\"\"\"\n",
    "    review = state[\"review_text\"]\n",
    "    if state[\"verbose\"]:\n",
    "        print(f\"Reviewbot received review: “{review}”\")\n",
    "    # no changes to state here\n",
    "    return {}\n",
    "\n",
    "#def fetch emotion definitions\n",
    "def fetch_emotion_definitions(state: ReviewState) -> dict:\n",
    "    \"\"\"Fetch Ekman emotion definitions\"\"\"\n",
    "    # Define the Ekman emotions and their definitions\n",
    "    emotion_definitions = {\n",
    "        \"anger\": \"A strong feeling of displeasure or hostility.\",\n",
    "        \"disgust\": \"A strong feeling of aversion or repulsion.\",\n",
    "        \"fear\": \"An unpleasant emotion caused by the belief that someone or something is dangerous.\",\n",
    "        \"joy\": \"A feeling of great pleasure and happiness.\",\n",
    "        \"sadness\": \"A state of emotional pain or unhappiness.\",\n",
    "        \"surprise\": \"A brief emotional state resulting from an unexpected event.\",\n",
    "        \"neutral\": \"Lacking strong emotion; neither positive nor negative.\"\n",
    "    }\n",
    "    \n",
    "    # Store the definitions in the state\n",
    "    state[\"emotion_definitions\"] = emotion_definitions\n",
    "    return {\"emotion_definitions\": emotion_definitions}\n",
    "\n",
    "#dynamic few-shotting technique, matches unseen reviews to the most similar few-shot examples\n",
    "def retrieve_few_shot_examples(state: ReviewState) -> dict:\n",
    "    \"\"\"\n",
    "    Embed the incoming review, compute cosine similarity against your pre-embedded few_shots,\n",
    "    and stash the top-5 (text, emotion) pairs into state['few_shot_examples'].\n",
    "    \"\"\"\n",
    "    # 1) Embed the new review\n",
    "    q_emb = embed(state[\"review_text\"]).reshape(1, -1).astype(\"float32\")\n",
    "    \n",
    "    # 2) Compute cosine similarities\n",
    "    sims = cosine_similarity(q_emb, emb_matrix)[0]  # shape (N,)\n",
    "    \n",
    "    # 3) Grab top-N indices\n",
    "    topk = sims.argsort()[-7:][::-1]\n",
    "    \n",
    "    new_examples = []\n",
    "    for i in topk:\n",
    "        if not (0 <= i < len(few_shots)):\n",
    "            # Optionally, log a warning here if an index is out of bounds,\n",
    "            # though with argsort this shouldn't happen if emb_matrix and few_shots are aligned.\n",
    "            # print(f\"Warning: Index i={i} is out of bounds for few_shots (len={len(few_shots)})\")\n",
    "            continue\n",
    "\n",
    "        current_shot_element = few_shots[i]\n",
    "\n",
    "        if not isinstance(current_shot_element, dict):\n",
    "            # Optionally, log an error or warning here\n",
    "            # print(f\"Warning: few_shots[{i}] is not a dictionary! It is a {type(current_shot_element)}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            text_val = current_shot_element[\"text\"]\n",
    "            emotion_val = current_shot_element[\"emotion\"]\n",
    "            \n",
    "            example_to_add = {\"text\": text_val, \"emotion\": emotion_val}\n",
    "            new_examples.append(example_to_add)\n",
    "\n",
    "        except KeyError as e:\n",
    "            # Optionally, log this error if it's critical\n",
    "            # print(f\"Error: KeyError occurred while processing: {current_shot_element}. Key: {e.args[0]}\")\n",
    "            # Depending on desired robustness, you might skip this example or re-raise\n",
    "            continue # Skip this example if a key is missing\n",
    "        except Exception as e:\n",
    "            # Optionally, log other unexpected errors\n",
    "            # print(f\"Error: An unexpected error occurred while processing: {current_shot_element}. Error: {e}\")\n",
    "            continue # Skip this example on other errors\n",
    "            \n",
    "    return {\"few_shot_examples\": new_examples}\n",
    "\n",
    "def classify_emotion(state: ReviewState) -> dict:\n",
    "    \"\"\"Use the LLM with few-shot context\"\"\"\n",
    "    definitions = state[\"emotion_definitions\"]\n",
    "    text = state[\"review_text\"]\n",
    "    # Format few-shot context\n",
    "    example_lines = \"\\n\".join(\n",
    "        f\"- “{ex['text']}” → {ex['emotion']}\"\n",
    "        for ex in state[\"few_shot_examples\"]\n",
    "        \n",
    "    )\n",
    "    #print(f\"Few-shot examples: {example_lines}\") used for testing\n",
    "    prompt = f\"\"\"\n",
    "You are a highly precise emotion-classification agent that excels at detecting emotion from text. \n",
    "you will be given a text / review and similar examples with labels \n",
    "\n",
    "Your task is to read a text / customer review, understand its content, and assign **one or more** of these emotions: \n",
    "anger, disgust, fear, joy, sadness, surprise, or neutral if no clear emotion is detected. \n",
    "\n",
    "the text:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "to better help you classify below are the definitions of each possible emotion:\n",
    "{definitions}\n",
    "\n",
    "and here are a few similar examples with Labels, to help you classify the above text / review:\n",
    "{example_lines}\n",
    "\n",
    "Now classify the text / review into one or more of:\n",
    "Anger, disgust, fear, joy, sadness, surprise or neutral if no clear emotion is detected.\n",
    "\n",
    "Respond **only** in JSON with:\n",
    "  \"emotion\": < a string or list of strings from the set above.\n",
    "\"\"\"\n",
    "    response_model = model.create_response_model(\n",
    "        \"FewShotEmotionClassification\",\n",
    "        {\n",
    "            \"emotion\": (\n",
    "                List[Literal[\"anger\",\"disgust\",\"fear\",\"joy\",\"sadness\",\"surprise\",\"neutral\"]],\n",
    "                Field(description=\"The chosen Ekman emotion(s)\")\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    resp = model.invoke(prompt, response_model=response_model)\n",
    "    return {\"emotion\": resp.emotion}\n",
    "\n",
    "def draft_response(state: ReviewState) -> dict:\n",
    "    \"\"\"Draft an empathetic reply based on the classified emotion\"\"\"\n",
    "    text = state[\"review_text\"]\n",
    "    emo  = state[\"emotion\"]\n",
    "    prompt = f\"\"\"A customer expresses **{emo}** in this review:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "You are a customer-service specialist with excellent empathy and brand voice consistency.  \n",
    "Given a review and its classified emotion(s), draft a short reply that:\n",
    "\n",
    "  - Acknowledges their {emo}, WITHOUT writing the specific emotions in the response,\n",
    "  - Addresses any concerns they raise,\n",
    "  - Invites further dialogue if needed.\n",
    "\"\"\"\n",
    "    resp = model.invoke(prompt)\n",
    "    return {\"draft_reply\": resp.answer}\n",
    "\n",
    "#showing all the states in the graph\n",
    "def notify_user(state: ReviewState) -> dict:\n",
    "    \"\"\"Present the final draft back to the support employee\"\"\"\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\"review : {state[\"review_text\"]}\")\n",
    "        print(f\"Review classified as: {state['emotion']}\")\n",
    "        print(\"Response:\")\n",
    "        print(state[\"draft_reply\"])\n",
    "        print(\"=\"*40 + \"\\n\")\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f84eb",
   "metadata": {},
   "source": [
    "No need for conditional edge as it is a linear system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bddc11",
   "metadata": {},
   "source": [
    "### Creating Graph of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "050e574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "review_graph = StateGraph(ReviewState)  # Initialize with your ReviewState\n",
    "\n",
    "# Add nodes\n",
    "review_graph.add_node(\"read review\",       read_review)\n",
    "review_graph.add_node(\"fetch emotion definitions\", fetch_emotion_definitions)\n",
    "review_graph.add_node(\"retrieve few-shot examples\", retrieve_few_shot_examples)\n",
    "review_graph.add_node(\"classify emotion\",  classify_emotion)\n",
    "review_graph.add_node(\"draft response\",    draft_response)\n",
    "review_graph.add_node(\"notify user\",      notify_user)\n",
    "\n",
    "# Wire up the edges\n",
    "# 1) START → read_review\n",
    "review_graph.add_edge(START, \"read review\")\n",
    "\n",
    "# 2) read_review → fetch_emotion_definitions\n",
    "review_graph.add_edge(\"read review\", \"fetch emotion definitions\")\n",
    "\n",
    "review_graph.add_edge(\"fetch emotion definitions\", \"retrieve few-shot examples\")\n",
    "\n",
    "\n",
    "review_graph.add_edge(\"retrieve few-shot examples\", \"classify emotion\")\n",
    "# 2) retrieve_few_shot_examples → classify_emotion\n",
    "\n",
    "# 3) classify_emotion → route_review\n",
    "review_graph.add_edge(\"classify emotion\", \"draft response\")\n",
    "\n",
    "#4 ) draft_response → notify_user\n",
    "review_graph.add_edge(\"draft response\", \"notify user\")\n",
    "\n",
    "# 5) notify_user → END\n",
    "review_graph.add_edge(\"notify user\", END)\n",
    "\n",
    "\n",
    "compiled_graph = review_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f40cdd",
   "metadata": {},
   "source": [
    "Print the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(compiled_graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0e29d",
   "metadata": {},
   "source": [
    "Testing the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1470e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate test texts/ reviews to pass to my review bot\n",
    "Example_text1 = \"I love this product! It works great and exceeded my expectations.\"\n",
    "Example_text2 = \"I am very disappointed with the service. It was not what I expected.\"\n",
    "Example_text3 = \"The product is okay, but it could be better. I expected more.\"\n",
    "Example_text4 = \"the service was nothing special. I was hoping for a better experience. but product was good.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e4e79",
   "metadata": {},
   "source": [
    "test if system is correctly categorizing the text and drafting a response & quality of the few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe083d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing review\n",
      "Reviewbot received review: “the service was nothing special. I was hoping for a better experience. but product was good.”\n",
      "\n",
      "========================================\n",
      "review : the service was nothing special. I was hoping for a better experience. but product was good.\n",
      "Review classified as: ['neutral']\n",
      "Response:\n",
      "Thank you for your feedback. We're glad to hear that you found the product good. We're sorry to hear that the service was not as exceptional as you hoped. We are constantly working to improve and would love to make things right. If there's anything specific you'd like us to address, please let us know.\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process the legitimate email\n",
    "print(\"\\nProcessing review\")\n",
    "test_examples = compiled_graph.invoke(\n",
    "    {\n",
    "        \"review_text\": Example_text4, # invoke the graph with the email text\n",
    "        \"verbose\": True,  # enable verbose output\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d864d",
   "metadata": {},
   "source": [
    "Evaluating the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d23961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emotion columns (ensure this matches your dataset and classify_emotion output)\n",
    "emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# 1. Load test data\n",
    "try:\n",
    "    df_test = pd.read_csv(\"data/ekman_test.csv\")\n",
    "    df_test = df_test[:50] #small dataset for testing the classification function\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: data/ekman_test.csv not found. Make sure the path is correct.\")\n",
    "    raise\n",
    "\n",
    "# Prepare lists for true and predicted labels\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c09915",
   "metadata": {},
   "source": [
    "checking split of different labels in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e76dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion distribution in the test set (df_test):\n",
      "anger: 74\n",
      "disgust: 14\n",
      "fear: 9\n",
      "joy: 207\n",
      "neutral: 140\n",
      "sadness: 45\n",
      "surprise: 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion distribution in the test set (df_test):\n",
      "anger: 150\n",
      "disgust: 25\n",
      "fear: 26\n",
      "joy: 412\n",
      "neutral: 269\n",
      "sadness: 93\n",
      "surprise: 119\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print count of each emotion in the test dataframe\n",
    "print(\"\\nEmotion distribution in the test set (df_test):\")\n",
    "for emotion in emotion_columns:\n",
    "    if emotion in df_test.columns:\n",
    "        count = df_test[emotion].sum()\n",
    "        print(f\"{emotion}: {count}\")\n",
    "    else:\n",
    "        print(f\"{emotion}: Column not found in df_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating classify_emotion function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test data: 100%|██████████| 50/50 [03:36<00:00,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for classify_emotion:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.14      0.20      0.17         5\n",
      "     disgust       0.11      1.00      0.20         1\n",
      "        fear       1.00      0.50      0.67         2\n",
      "         joy       0.73      0.58      0.65        19\n",
      "     neutral       0.57      0.72      0.63        18\n",
      "     sadness       0.33      0.33      0.33         6\n",
      "    surprise       0.33      0.60      0.43         5\n",
      "\n",
      "   micro avg       0.46      0.57      0.51        56\n",
      "   macro avg       0.46      0.56      0.44        56\n",
      "weighted avg       0.55      0.57      0.54        56\n",
      " samples avg       0.46      0.60      0.50        56\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evaluating classify_emotion function...\")\n",
    "\n",
    "for index, row in tqdm(df_test.iterrows(), total=df_test.shape[0], desc=\"Processing test data\"):\n",
    "    text = row['text']\n",
    "    \n",
    "    # Prepare true labels for this sample\n",
    "    true_emotions_for_sample = [col for col in emotion_columns if col in row and row[col] == 1]\n",
    "    all_true_labels.append(true_emotions_for_sample)\n",
    "    \n",
    "    # Prepare state for classify_emotion\n",
    "    # ReviewState is defined in a previous cell\n",
    "    current_state = ReviewState(\n",
    "        review_text=text,\n",
    "        few_shot_examples=[], \n",
    "        emotion=None,         \n",
    "        draft_reply=None,\n",
    "        verbose=False         \n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        retrived_state_update = fetch_emotion_definitions(current_state)\n",
    "        current_state.update(retrived_state_update)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during fetch_emotion_definitions for text: '{text[:50]}...': {e}\")\n",
    "        all_pred_labels.append([]) # Append empty list for this sample on error\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        retrieved_state_update = retrieve_few_shot_examples(current_state)\n",
    "        current_state.update(retrieved_state_update)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during retrieve_few_shot_examples for text: '{text[:50]}...': {e}\")\n",
    "        all_pred_labels.append([]) # Append empty list for this sample on error\n",
    "        continue\n",
    "\n",
    "    # Call classify_emotion\n",
    "    try:\n",
    "        classification_result = classify_emotion(current_state)\n",
    "        predicted_emotions_for_sample = classification_result.get('emotion', [])\n",
    "        \n",
    "        if isinstance(predicted_emotions_for_sample, str): # Ensure it's a list\n",
    "            predicted_emotions_for_sample = [predicted_emotions_for_sample]\n",
    "        \n",
    "        all_pred_labels.append(predicted_emotions_for_sample)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during classify_emotion for text: '{text[:50]}...': {e}\")\n",
    "        all_pred_labels.append([]) # Append empty list for this sample on error\n",
    "        continue\n",
    "\n",
    "# 4. Evaluate\n",
    "# Initialize MultiLabelBinarizer with all possible emotion classes\n",
    "mlb = MultiLabelBinarizer(classes=emotion_columns)\n",
    "\n",
    "# Fit on all possible labels to ensure consistent encoding, then transform\n",
    "mlb.fit([emotion_columns]) # Fit with all known classes\n",
    "y_true_binarized = mlb.transform(all_true_labels)\n",
    "y_pred_binarized = mlb.transform(all_pred_labels)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report for classify_emotion:\")\n",
    "report = classification_report(y_true_binarized, y_pred_binarized, target_names=mlb.classes_, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "# Calculate overall metrics\n",
    "precision_micro = precision_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "recall_micro = recall_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "f1_micro = f1_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "\n",
    "precision_macro = precision_score(y_true_binarized, y_pred_binarized, average='macro', zero_division=0)\n",
    "recall_macro = recall_score(y_true_binarized, y_pred_binarized, average='macro', zero_division=0)\n",
    "f1_macro = f1_score(y_true_binarized, y_pred_binarized, average='macro', zero_division=0)\n",
    "\n",
    "precision_weighted = precision_score(y_true_binarized, y_pred_binarized, average='weighted', zero_division=0)\n",
    "recall_weighted = recall_score(y_true_binarized, y_pred_binarized, average='weighted', zero_division=0)\n",
    "f1_weighted = f1_score(y_true_binarized, y_pred_binarized, average='weighted', zero_division=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d49e8810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.30\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_true_binarized, y_pred_binarized)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b680305",
   "metadata": {},
   "source": [
    "Testing the system on new self procured dataset of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "114926ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#defineing the columns\n",
    "emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# 1. Load new test data\n",
    "new_test_csv_path = os.path.join(\"data\", \"reviews.csv\")\n",
    "try:\n",
    "    df_reviews_test = pd.read_csv(new_test_csv_path, delimiter=';')\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {new_test_csv_path} not found. Make sure the path is correct.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {new_test_csv_path}: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf723016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating classify_emotion function on data\\reviews.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reviews.csv: 100%|██████████| 79/79 [03:17<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for classify_emotion on data\\reviews.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.83      0.80      0.82        25\n",
      "     disgust       0.33      0.83      0.48         6\n",
      "        fear       0.00      0.00      0.00         2\n",
      "         joy       0.86      1.00      0.92        36\n",
      "     neutral       0.86      0.50      0.63        12\n",
      "     sadness       0.44      0.73      0.55        11\n",
      "    surprise       0.33      0.20      0.25        10\n",
      "\n",
      "   micro avg       0.69      0.75      0.72       102\n",
      "   macro avg       0.52      0.58      0.52       102\n",
      "weighted avg       0.71      0.75      0.71       102\n",
      " samples avg       0.73      0.79      0.73       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare lists for true and predicted labels\n",
    "all_true_labels_reviews_csv = []\n",
    "all_pred_labels_reviews_csv = []\n",
    "\n",
    "print(f\"\\nEvaluating classify_emotion function on {new_test_csv_path}...\")\n",
    "\n",
    "\n",
    "for index, row in tqdm(df_reviews_test.iterrows(), total=df_reviews_test.shape[0], desc=\"Processing reviews.csv\"):\n",
    "    text = row['Text']\n",
    "    \n",
    "    # Prepare true labels for this sample\n",
    "    # This assumes  CSV has columns named after emotions (e.g., 'joy', 'anger')\n",
    "    true_emotions_for_sample = [col for col in emotion_columns if col in row and row[col] == 1]\n",
    "    all_true_labels_reviews_csv.append(true_emotions_for_sample)\n",
    "    \n",
    "    # Prepare state for classify_emotion\n",
    "    # ReviewState is defined in a previous cell\n",
    "    current_state = ReviewState(\n",
    "        review_text=text,\n",
    "        few_shot_examples=[], \n",
    "        emotion=None,         \n",
    "        draft_reply=None,\n",
    "        verbose=False         \n",
    "    )\n",
    "    try:\n",
    "        retrived_state_update = fetch_emotion_definitions(current_state)\n",
    "        current_state.update(retrived_state_update)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during fetch_emotion_definitions for text: '{text[:50]}...': {e}\")\n",
    "        all_pred_labels.append([]) # Append empty list for this sample on error\n",
    "        continue\n",
    "\n",
    "    # Call retrieve_few_shot_examples (defined in cell id: 0bc7bdc9)\n",
    "    try:\n",
    "        retrieved_state_update = retrieve_few_shot_examples(current_state)\n",
    "        current_state.update(retrieved_state_update) # Update the state directly\n",
    "    except Exception as e:\n",
    "        print(f\"Error during retrieve_few_shot_examples for text: '{text[:50]}...': {e}\")\n",
    "        all_pred_labels_reviews_csv.append([]) # Append empty list for this sample on error\n",
    "        continue\n",
    "\n",
    "    # Call classify_emotion (defined in cell id: 0bc7bdc9)\n",
    "    try:\n",
    "        classification_result = classify_emotion(current_state)\n",
    "        predicted_emotions_for_sample = classification_result.get('emotion', [])\n",
    "        \n",
    "        if isinstance(predicted_emotions_for_sample, str): # Ensure it's a list\n",
    "            predicted_emotions_for_sample = [predicted_emotions_for_sample]\n",
    "        \n",
    "        all_pred_labels_reviews_csv.append(predicted_emotions_for_sample)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during classify_emotion for text: '{text[:50]}...': {e}\")\n",
    "        all_pred_labels_reviews_csv.append([]) # Append empty list for this sample on error\n",
    "        continue\n",
    "\n",
    "# 4. Evaluate\n",
    "mlb_reviews_csv = MultiLabelBinarizer(classes=emotion_columns)\n",
    "\n",
    "# Fit on all possible labels to ensure consistent encoding, then transform\n",
    "mlb_reviews_csv.fit([emotion_columns]) \n",
    "y_true_binarized_reviews_csv = mlb_reviews_csv.transform(all_true_labels_reviews_csv)\n",
    "y_pred_binarized_reviews_csv = mlb_reviews_csv.transform(all_pred_labels_reviews_csv)\n",
    "\n",
    "# Print classification report\n",
    "print(f\"\\nClassification Report for classify_emotion on {new_test_csv_path}:\")\n",
    "report_reviews_csv = classification_report(y_true_binarized_reviews_csv, y_pred_binarized_reviews_csv, target_names=mlb_reviews_csv.classes_, zero_division=0)\n",
    "print(report_reviews_csv)\n",
    "\n",
    "# Calculate overall metrics\n",
    "precision_micro_reviews_csv = precision_score(y_true_binarized_reviews_csv, y_pred_binarized_reviews_csv, average='micro', zero_division=0)\n",
    "recall_micro_reviews_csv = recall_score(y_true_binarized_reviews_csv, y_pred_binarized_reviews_csv, average='micro', zero_division=0)\n",
    "f1_micro_reviews_csv = f1_score(y_true_binarized_reviews_csv, y_pred_binarized_reviews_csv, average='micro', zero_division=0)\n",
    "\n",
    "precision_macro_reviews_csv = precision_score(y_true_binarized_reviews_csv, y_pred_binarized_reviews_csv, average='macro', zero_division=0)\n",
    "recall_macro_reviews_csv = recall_score(y_true_binarized_reviews_csv, y_pred_binarized_reviews_csv, average='macro', zero_division=0)\n",
    "f1_macro_reviews_csv = f1_score(y_true_binarized_reviews_csv, y_pred_binarized_reviews_csv, average='macro', zero_division=0)\n",
    "\n",
    "precision_weighted_reviews_csv = precision_score(y_true_binarized_reviews_csv, y_pred_binarized_reviews_csv, average='weighted', zero_division=0)\n",
    "recall_weighted_reviews_csv = recall_score(y_true_binarized_reviews_csv, y_pred_binarized_reviews_csv, average='weighted', zero_division=0)\n",
    "f1_weighted_reviews_csv = f1_score(y_true_binarized_reviews_csv, y_pred_binarized_reviews_csv, average='weighted', zero_division=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
